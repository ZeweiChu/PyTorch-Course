{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq, Attention\n",
    "\n",
    "褚则伟 zeweichu@gmail.com\n",
    "\n",
    "在这份notebook当中，我们会(尽可能)复现Luong的attention模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更多阅读\n",
    "\n",
    "#### 课件\n",
    "- [cs224d](http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf)\n",
    "\n",
    "\n",
    "#### 论文\n",
    "- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025?context=cs)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1406.1078)\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "#### PyTorch代码\n",
    "- [seq2seq-tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)\n",
    "- [Tutorial from Ben Trevett](https://github.com/bentrevett/pytorch-seq2seq)\n",
    "- [IBM seq2seq](https://github.com/IBM/pytorch-seq2seq)\n",
    "- [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入中英文数据\n",
    "- 英文我们使用nltk的word tokenizer来分词，并且使用小写字母\n",
    "- 中文我们直接使用单个汉字作为基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            \n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            # split chinese sentence into characters\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn\n",
    "\n",
    "train_file = \"nmt/en-cn/train.txt\"\n",
    "dev_file = \"nmt/en-cn/dev.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建单词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1\n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 2\n",
    "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
    "    word_dict[\"UNK\"] = UNK_IDX\n",
    "    word_dict[\"PAD\"] = PAD_IDX\n",
    "    return word_dict, total_words\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把单词全部转变成数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    '''\n",
    "        Encode the sequences. \n",
    "    '''\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "\n",
    "    # sort sentences by english lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把全部句子分成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches\n",
    "\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths #x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据全部处理完成，现在我们开始构建seq2seq模型\n",
    "\n",
    "#### Encoder\n",
    "- Encoder模型的任务是把输入文字传入embedding层和GRU层，转换成一些hidden states作为后续的context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
    "\n",
    "        return out, hid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luong Attention\n",
    "- 根据context vectors和当前的输出hidden states，计算输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "\n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
    "        \n",
    "    def forward(self, output, context, mask):\n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        # context: batch_size, context_len, enc_hidden_size\n",
    "    \n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        \n",
    "        context_in = self.linear_in(context.view(batch_size*input_len, -1)).view(                batch_size, input_len, -1) # batch_size, output_len, dec_hidden_size\n",
    "        attn = torch.bmm(output, context_in.transpose(1,2)) # batch_size, output_len, context_len\n",
    "\n",
    "        \n",
    "        attn.data.masked_fill(mask, -1e6)\n",
    "\n",
    "        attn = F.softmax(attn, dim=2) # batch_size, output_len, context_len\n",
    "\n",
    "        context = torch.bmm(attn, context) # batch_size, output_len, enc_hidden_size\n",
    "        \n",
    "        output = torch.cat((context, output), dim=2) # batch_size, output_len, hidden_size*2\n",
    "\n",
    "        \n",
    "        output = output.view(batch_size*output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "- decoder会根据已经翻译的句子内容，和context vectors，来决定下一个输出的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
    "        mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "        \n",
    "        \n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        \n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
    "\n",
    "        # code.interact(local=locals())\n",
    "        output, attn = self.attention(output_seq, ctx, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "        \n",
    "        return output, hid, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq\n",
    "- 最后我们构建Seq2Seq模型把encoder, attention, decoder串到一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "#             print(output.shape, output.max(1)[1])\n",
    "            preds.append(output.max(2)[1].view(batch_size, 1))\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "en_vocab_size = len(en_dict)\n",
    "cn_vocab_size = len(cn_dict)\n",
    "embed_size = hidden_size = 100\n",
    "dropout = 0.2\n",
    "\n",
    "encoder = Encoder(vocab_size=en_vocab_size, \n",
    "                  embed_size=embed_size, \n",
    "                  enc_hidden_size=hidden_size,\n",
    "                  dec_hidden_size=hidden_size,\n",
    "                  dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_vocab_size, \n",
    "                  embed_size=embed_size, \n",
    "                  enc_hidden_size=hidden_size,\n",
    "                  dec_hidden_size=hidden_size,\n",
    "                  dropout=dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "crit = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_lengths, mb_y, mb_y_lengths) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).long().to(device)\n",
    "            mb_x_lengths = torch.from_numpy(mb_x_lengths).long().to(device)\n",
    "            mb_input = torch.from_numpy(mb_y[:,:-1]).long().to(device)\n",
    "            mb_out = torch.from_numpy(mb_y[:, 1:]).long().to(device)\n",
    "            mb_y_lengths = torch.from_numpy(mb_y_lengths-1).long().to(device)\n",
    "            mb_y_lengths[mb_y_lengths <= 0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_lengths, mb_input, mb_y_lengths)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_lengths.max().item(), device=device)[None, :] < mb_y_lengths[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            loss = crit(mb_pred, mb_out, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_lengths).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "\n",
    "    print(\"evaluation loss\", total_loss/total_num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, data, num_epochs=30):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_num_words = total_loss = 0.\n",
    "        model.train()\n",
    "        for it, (mb_x, mb_x_lengths, mb_y, mb_y_lengths) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).long().to(device)\n",
    "            mb_x_lengths = torch.from_numpy(mb_x_lengths).long().to(device)\n",
    "            mb_input = torch.from_numpy(mb_y[:,:-1]).long().to(device)\n",
    "            mb_out = torch.from_numpy(mb_y[:, 1:]).long().to(device)\n",
    "            mb_y_lengths = torch.from_numpy(mb_y_lengths-1).long().to(device)\n",
    "            mb_y_lengths[mb_y_lengths <= 0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_lengths, mb_input, mb_y_lengths)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_lengths.max().item(), device=device)[None, :] < mb_y_lengths[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            loss = crit(mb_pred, mb_out, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_lengths).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "\n",
    "            if it % 100 == 0:\n",
    "                print(\"epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "        print(\"epoch\", epoch, \"training loss\", total_loss/total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"evaluating on dev...\")\n",
    "            evaluate(model, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration 0 loss 8.085762023925781\n",
      "epoch 0 iteration 100 loss 5.319369316101074\n",
      "epoch 0 iteration 200 loss 4.981622695922852\n",
      "epoch 0 training loss 5.481902322175141\n",
      "evaluating on dev...\n",
      "evaluation loss 5.057798780270827\n",
      "epoch 1 iteration 0 loss 5.040606498718262\n",
      "epoch 1 iteration 100 loss 4.838320732116699\n",
      "epoch 1 iteration 200 loss 4.481621265411377\n",
      "epoch 1 training loss 4.8332787057724955\n",
      "epoch 2 iteration 0 loss 4.487977981567383\n",
      "epoch 2 iteration 100 loss 4.352450847625732\n",
      "epoch 2 iteration 200 loss 4.13072395324707\n",
      "epoch 2 training loss 4.397700447852463\n",
      "epoch 3 iteration 0 loss 4.078022003173828\n",
      "epoch 3 iteration 100 loss 4.01767635345459\n",
      "epoch 3 iteration 200 loss 3.8409407138824463\n",
      "epoch 3 training loss 4.068776939315962\n",
      "epoch 4 iteration 0 loss 3.7688279151916504\n",
      "epoch 4 iteration 100 loss 3.723728656768799\n",
      "epoch 4 iteration 200 loss 3.567530870437622\n",
      "epoch 4 training loss 3.811968992090653\n",
      "epoch 5 iteration 0 loss 3.533092737197876\n",
      "epoch 5 iteration 100 loss 3.505263566970825\n",
      "epoch 5 iteration 200 loss 3.3965344429016113\n",
      "epoch 5 training loss 3.6039628209416334\n",
      "evaluating on dev...\n",
      "evaluation loss 3.628216229238098\n",
      "epoch 6 iteration 0 loss 3.3119990825653076\n",
      "epoch 6 iteration 100 loss 3.282820224761963\n",
      "epoch 6 iteration 200 loss 3.207045555114746\n",
      "epoch 6 training loss 3.428878677700353\n",
      "epoch 7 iteration 0 loss 3.171563148498535\n",
      "epoch 7 iteration 100 loss 3.0975048542022705\n",
      "epoch 7 iteration 200 loss 3.0484087467193604\n",
      "epoch 7 training loss 3.283246673536695\n",
      "epoch 8 iteration 0 loss 3.0407843589782715\n",
      "epoch 8 iteration 100 loss 2.9599945545196533\n",
      "epoch 8 iteration 200 loss 2.951413631439209\n",
      "epoch 8 training loss 3.158410066431942\n",
      "epoch 9 iteration 0 loss 2.927349805831909\n",
      "epoch 9 iteration 100 loss 2.8311045169830322\n",
      "epoch 9 iteration 200 loss 2.838622570037842\n",
      "epoch 9 training loss 3.0455910512848643\n",
      "epoch 10 iteration 0 loss 2.7916982173919678\n",
      "epoch 10 iteration 100 loss 2.7216992378234863\n",
      "epoch 10 iteration 200 loss 2.755091667175293\n",
      "epoch 10 training loss 2.9456758171521393\n",
      "evaluating on dev...\n",
      "evaluation loss 3.24746060556361\n",
      "epoch 11 iteration 0 loss 2.7126755714416504\n",
      "epoch 11 iteration 100 loss 2.653618812561035\n",
      "epoch 11 iteration 200 loss 2.637237310409546\n",
      "epoch 11 training loss 2.85465293327523\n",
      "epoch 12 iteration 0 loss 2.6224520206451416\n",
      "epoch 12 iteration 100 loss 2.5167288780212402\n",
      "epoch 12 iteration 200 loss 2.54325270652771\n",
      "epoch 12 training loss 2.7734754377962814\n",
      "epoch 13 iteration 0 loss 2.5449390411376953\n",
      "epoch 13 iteration 100 loss 2.4622833728790283\n",
      "epoch 13 iteration 200 loss 2.4742157459259033\n",
      "epoch 13 training loss 2.6992580153905625\n",
      "epoch 14 iteration 0 loss 2.481992244720459\n",
      "epoch 14 iteration 100 loss 2.378389596939087\n",
      "epoch 14 iteration 200 loss 2.3833160400390625\n",
      "epoch 14 training loss 2.6257236516458606\n",
      "epoch 15 iteration 0 loss 2.4211721420288086\n",
      "epoch 15 iteration 100 loss 2.3029396533966064\n",
      "epoch 15 iteration 200 loss 2.344189167022705\n",
      "epoch 15 training loss 2.5635952320394573\n",
      "evaluating on dev...\n",
      "evaluation loss 3.101286390976075\n",
      "epoch 16 iteration 0 loss 2.3324387073516846\n",
      "epoch 16 iteration 100 loss 2.1891722679138184\n",
      "epoch 16 iteration 200 loss 2.2504241466522217\n",
      "epoch 16 training loss 2.501802640199787\n",
      "epoch 17 iteration 0 loss 2.2990927696228027\n",
      "epoch 17 iteration 100 loss 2.155005693435669\n",
      "epoch 17 iteration 200 loss 2.244598150253296\n",
      "epoch 17 training loss 2.445841398777448\n",
      "epoch 18 iteration 0 loss 2.2480554580688477\n",
      "epoch 18 iteration 100 loss 2.084474563598633\n",
      "epoch 18 iteration 200 loss 2.1368367671966553\n",
      "epoch 18 training loss 2.3917858462271795\n",
      "epoch 19 iteration 0 loss 2.176265239715576\n",
      "epoch 19 iteration 100 loss 2.0467331409454346\n",
      "epoch 19 iteration 200 loss 2.096245288848877\n",
      "epoch 19 training loss 2.3402440253858687\n",
      "epoch 20 iteration 0 loss 2.151754140853882\n",
      "epoch 20 iteration 100 loss 2.01328182220459\n",
      "epoch 20 iteration 200 loss 2.0409457683563232\n",
      "epoch 20 training loss 2.291819297698111\n",
      "evaluating on dev...\n",
      "evaluation loss 3.027398743661337\n",
      "epoch 21 iteration 0 loss 2.0911855697631836\n",
      "epoch 21 iteration 100 loss 1.9238929748535156\n",
      "epoch 21 iteration 200 loss 1.9701272249221802\n",
      "epoch 21 training loss 2.2482194887573073\n",
      "epoch 22 iteration 0 loss 2.0453591346740723\n",
      "epoch 22 iteration 100 loss 1.8722518682479858\n",
      "epoch 22 iteration 200 loss 1.9823880195617676\n",
      "epoch 22 training loss 2.207148945490473\n",
      "epoch 23 iteration 0 loss 1.9964138269424438\n",
      "epoch 23 iteration 100 loss 1.8535857200622559\n",
      "epoch 23 iteration 200 loss 1.9315838813781738\n",
      "epoch 23 training loss 2.162252000802971\n",
      "epoch 24 iteration 0 loss 1.9458106756210327\n",
      "epoch 24 iteration 100 loss 1.7884819507598877\n",
      "epoch 24 iteration 200 loss 1.8921432495117188\n",
      "epoch 24 training loss 2.1247135202012566\n",
      "epoch 25 iteration 0 loss 1.9540257453918457\n",
      "epoch 25 iteration 100 loss 1.7476329803466797\n",
      "epoch 25 iteration 200 loss 1.8080668449401855\n",
      "epoch 25 training loss 2.086875990794326\n",
      "evaluating on dev...\n",
      "evaluation loss 2.99701335194927\n",
      "epoch 26 iteration 0 loss 1.8884531259536743\n",
      "epoch 26 iteration 100 loss 1.720410704612732\n",
      "epoch 26 iteration 200 loss 1.8002591133117676\n",
      "epoch 26 training loss 2.0519073400427885\n",
      "epoch 27 iteration 0 loss 1.851972222328186\n",
      "epoch 27 iteration 100 loss 1.6909284591674805\n",
      "epoch 27 iteration 200 loss 1.7638009786605835\n",
      "epoch 27 training loss 2.0167982192911897\n",
      "epoch 28 iteration 0 loss 1.8264316320419312\n",
      "epoch 28 iteration 100 loss 1.6786623001098633\n",
      "epoch 28 iteration 200 loss 1.7356207370758057\n",
      "epoch 28 training loss 1.9889577699922467\n",
      "epoch 29 iteration 0 loss 1.7878819704055786\n",
      "epoch 29 iteration 100 loss 1.632318377494812\n",
      "epoch 29 iteration 200 loss 1.7153290510177612\n",
      "epoch 29 training loss 1.9556142197439679\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你 你 你 你 你 你 你 你 你 你 你 说 说 说 说 说 你 你 你 你 你 你 你 你 你 你 你 你 你 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你 你 这 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 说 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "當 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "那 那 那 那 那 这 这 这 这 这 这 说 说 说 多 多 多 多 多 多 多 多 多 多 多 多 多 ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "你 在 在 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 在 在 在 在 在 在 在 在 在 在 多 多 多 多 多 多 多 到 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 这 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "它 它 它 它 它 这 这 说 说 说 说 说 说 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "汤 他 他 他 他 他 他 他 把 把 把 把 把 把 把 把 把 把 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 到 到 到 到 到 到 到\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "即 即 即 就 就 就 马 马 把 把 把 把 把 马 马 马 马 一 一 一 一 一 一 一 一 一 一 一 一 一 一 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "如 你 你 你 是 是 在 在 在 在 说 说 说 说 说 说 说 说 说 ， ， ， ， ， ， ， ， ， ， ， ， ， ， ， ， ， 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "没 没 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 不 ， ， ， ， ， ， ， ， 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "请 请 即 把 把 把 把 把 把 把 把 把 把 把 把 把 把 把 把 把 把 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到 到\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤 汤 汤 汤 汤 汤 汤 汤 汤 汤 汤 汤 这 这 这 这 这 这 这 这 这 这 这 这 了 了 了 了 了 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "汤 如 知 别 别 别 别 别 别 说 说 说 说 说 说 说 多 多 多 多 多 多 多 多 多 多 多 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 更 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "即 即 这 这 这 这 把 把 把 把 把 把 把 把 把 把 把 把 把 一 一 一 一 一 一 一 一 一 一 一 一 一 一 一 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_dev(i):\n",
    "    model.eval()\n",
    "    \n",
    "    en_sent = \" \".join([inv_en_dict[word] for word in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    print(\" \".join([inv_cn_dict[word] for word in dev_cn[i]]))\n",
    "\n",
    "    sent = nltk.word_tokenize(en_sent.lower())\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "    mb_x = torch.Tensor([[en_dict.get(w, 0) for w in sent]]).long().to(device)\n",
    "    mb_x_len = torch.Tensor([len(sent)]).long().to(device)\n",
    "    \n",
    "    translation, attention = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\" \".join(translation))\n",
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 没有Attention的版本\n",
    "下面是一个更简单的没有Attention的encoder decoder模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        return out, hid[[-1]]\n",
    "\n",
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        output = F.log_softmax(self.out(output_seq), -1)\n",
    "        \n",
    "        return output, hid\n",
    "    \n",
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "#         print(hid.size())\n",
    "        output, hid = self.decoder(y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, None\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            preds.append(output.max(2)[1].view(batch_size, 1))\n",
    "        return torch.cat(preds, 1), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dropout=0.1\n",
    "\n",
    "en_vocab_size = len(en_dict)\n",
    "cn_vocab_size = len(cn_dict)\n",
    "embed_size = hidden_size = 100\n",
    "encoder = PlainEncoder(vocab_size=en_vocab_size, \n",
    "                  hidden_size=hidden_size,\n",
    "                dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_vocab_size, \n",
    "                  hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = PlainSeq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "crit = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration 0 loss 8.066805839538574\n",
      "epoch 0 iteration 100 loss 5.2180585861206055\n",
      "epoch 0 iteration 200 loss 4.785095691680908\n",
      "epoch 0 training loss 5.434948407335765\n",
      "evaluating on dev...\n",
      "evaluation loss 4.812285970985656\n",
      "epoch 1 iteration 0 loss 4.732585906982422\n",
      "epoch 1 iteration 100 loss 4.509240627288818\n",
      "epoch 1 iteration 200 loss 4.226128578186035\n",
      "epoch 1 training loss 4.535631854416315\n",
      "epoch 2 iteration 0 loss 4.161963939666748\n",
      "epoch 2 iteration 100 loss 4.053190231323242\n",
      "epoch 2 iteration 200 loss 3.901448965072632\n",
      "epoch 2 training loss 4.11700074522156\n",
      "epoch 3 iteration 0 loss 3.8422834873199463\n",
      "epoch 3 iteration 100 loss 3.728240966796875\n",
      "epoch 3 iteration 200 loss 3.6542489528656006\n",
      "epoch 3 training loss 3.837974398165313\n",
      "epoch 4 iteration 0 loss 3.5797362327575684\n",
      "epoch 4 iteration 100 loss 3.4697306156158447\n",
      "epoch 4 iteration 200 loss 3.448756694793701\n",
      "epoch 4 training loss 3.629842438550507\n",
      "epoch 5 iteration 0 loss 3.388706922531128\n",
      "epoch 5 iteration 100 loss 3.29595947265625\n",
      "epoch 5 iteration 200 loss 3.299004077911377\n",
      "epoch 5 training loss 3.4671562550750767\n",
      "evaluating on dev...\n",
      "evaluation loss 3.5971120786795856\n",
      "epoch 6 iteration 0 loss 3.2295641899108887\n",
      "epoch 6 iteration 100 loss 3.1434402465820312\n",
      "epoch 6 iteration 200 loss 3.1823387145996094\n",
      "epoch 6 training loss 3.3338620683850886\n",
      "epoch 7 iteration 0 loss 3.1095640659332275\n",
      "epoch 7 iteration 100 loss 3.0197503566741943\n",
      "epoch 7 iteration 200 loss 3.0362844467163086\n",
      "epoch 7 training loss 3.2188462721944746\n",
      "epoch 8 iteration 0 loss 2.9785873889923096\n",
      "epoch 8 iteration 100 loss 2.9063560962677\n",
      "epoch 8 iteration 200 loss 2.946014404296875\n",
      "epoch 8 training loss 3.118874704909087\n",
      "epoch 9 iteration 0 loss 2.89678692817688\n",
      "epoch 9 iteration 100 loss 2.8090906143188477\n",
      "epoch 9 iteration 200 loss 2.8518054485321045\n",
      "epoch 9 training loss 3.0323294463905035\n",
      "epoch 10 iteration 0 loss 2.8177437782287598\n",
      "epoch 10 iteration 100 loss 2.7316508293151855\n",
      "epoch 10 iteration 200 loss 2.7544913291931152\n",
      "epoch 10 training loss 2.953707904513929\n",
      "evaluating on dev...\n",
      "evaluation loss 3.332757036067659\n",
      "epoch 11 iteration 0 loss 2.7278811931610107\n",
      "epoch 11 iteration 100 loss 2.6419429779052734\n",
      "epoch 11 iteration 200 loss 2.709653854370117\n",
      "epoch 11 training loss 2.881760058295125\n",
      "epoch 12 iteration 0 loss 2.6682941913604736\n",
      "epoch 12 iteration 100 loss 2.572019577026367\n",
      "epoch 12 iteration 200 loss 2.6517350673675537\n",
      "epoch 12 training loss 2.8167860571869463\n",
      "epoch 13 iteration 0 loss 2.6066031455993652\n",
      "epoch 13 iteration 100 loss 2.520965337753296\n",
      "epoch 13 iteration 200 loss 2.585294485092163\n",
      "epoch 13 training loss 2.757839826306445\n",
      "epoch 14 iteration 0 loss 2.5390865802764893\n",
      "epoch 14 iteration 100 loss 2.4208672046661377\n",
      "epoch 14 iteration 200 loss 2.5385398864746094\n",
      "epoch 14 training loss 2.7026833540259747\n",
      "epoch 15 iteration 0 loss 2.486100673675537\n",
      "epoch 15 iteration 100 loss 2.409888982772827\n",
      "epoch 15 iteration 200 loss 2.462641477584839\n",
      "epoch 15 training loss 2.647877219646653\n",
      "evaluating on dev...\n",
      "evaluation loss 3.2293755202467893\n",
      "epoch 16 iteration 0 loss 2.448392868041992\n",
      "epoch 16 iteration 100 loss 2.3404362201690674\n",
      "epoch 16 iteration 200 loss 2.3956198692321777\n",
      "epoch 16 training loss 2.5996115741888426\n",
      "epoch 17 iteration 0 loss 2.404669761657715\n",
      "epoch 17 iteration 100 loss 2.306980609893799\n",
      "epoch 17 iteration 200 loss 2.386343240737915\n",
      "epoch 17 training loss 2.556201184917513\n",
      "epoch 18 iteration 0 loss 2.346625566482544\n",
      "epoch 18 iteration 100 loss 2.264592170715332\n",
      "epoch 18 iteration 200 loss 2.362499713897705\n",
      "epoch 18 training loss 2.5118211467128777\n",
      "epoch 19 iteration 0 loss 2.311363458633423\n",
      "epoch 19 iteration 100 loss 2.184628486633301\n",
      "epoch 19 iteration 200 loss 2.3218424320220947\n",
      "epoch 19 training loss 2.4721470377498176\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS a friend of mine is studying abroad . EOS\n",
      "BOS 我 有 一 位 朋 友 在 國 外 留 學 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS i get along with my younger brother . EOS\n",
      "BOS 我 與 我 的 弟 弟 相 處 融 洽 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS this ticket is good for three days . EOS\n",
      "BOS 這 張 票 的 有 效 期 是 三 天 。 EOS\n",
      "我 我 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這\n",
      "\n",
      "BOS i gave careful consideration to the problem . EOS\n",
      "BOS 我 仔 細 地 考 慮 了 這 個 問 題 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS jealousy was the UNK for the murder . EOS\n",
      "BOS 嫉 妒 是 謀 殺 的 動 機 。 EOS\n",
      "我 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他\n",
      "\n",
      "BOS i do not allow sleeping in class . EOS\n",
      "BOS 我 不 允 许 有 人 在 课 上 睡 觉 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS i made some mistakes on the test . EOS\n",
      "BOS 我 在 考 试 时 犯 了 些 错 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS move the chair UNK to the desk . EOS\n",
      "BOS 把 椅 子 UNK 一 UNK 靠 近 桌 子 。 EOS\n",
      "所 當 當 當 目 目 目 目 目 目 目 目 當 當 當 當 當 當 當 當 當 當 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他\n",
      "\n",
      "BOS he had no luck in finding work . EOS\n",
      "BOS 他 不 幸 找 不 到 工 作 。 EOS\n",
      "因 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他\n",
      "\n",
      "BOS my brother is a high school student . EOS\n",
      "BOS 我 哥 哥 是 個 高 中 生 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS tom did n't write back to mary . EOS\n",
      "BOS 汤 姆 没 给 玛 丽 写 回 复 。 EOS\n",
      "汤 汤 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他\n",
      "\n",
      "BOS the meaning of this sentence is UNK . EOS\n",
      "BOS 这 句 句 子 意 思 模 糊 。 EOS\n",
      "這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這 這\n",
      "\n",
      "BOS i try not to think about it . EOS\n",
      "BOS 我 試 著 不 去 想 了 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS i do n't worry about the risk . EOS\n",
      "BOS 我 不 担 心 风 险 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS i 'm as hungry as a horse . EOS\n",
      "BOS 我 餓 得 像 匹 馬 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS we 'll save a seat for you . EOS\n",
      "BOS 我 们 会 给 你 留 个 位 置 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS it was been raining since early morning . EOS\n",
      "BOS 從 清 晨 開 始 一 直 下 雨 。 EOS\n",
      "今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 今 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS i had a nice chat with her . EOS\n",
      "BOS 我 和 她 聊 得 很 愉 快 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他 他\n",
      "\n",
      "BOS i put the money into the safe . EOS\n",
      "BOS 我 把 錢 放 入 保 險 櫃 裡 。 EOS\n",
      "我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "BOS have you ever climbed mt . UNK ? EOS\n",
      "BOS 你 曾 爬 過 槍 岳 嗎 ? EOS\n",
      "你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你 你\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000,1020):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
